WEBVTT
Kind: captions
Language: en

00:00:02.920 --> 00:00:05.121
My name is Yaroslav Halchenko

00:00:05.121 --> 00:00:09.750
and I am talking to you from Dartmouth about DataLad project.

00:00:10.299 --> 00:00:14.099
This project was initiated by me and Michael Hanke from Germany

00:00:14.099 --> 00:00:18.141
and we had successful few years of collaboration.

00:00:18.141 --> 00:00:22.000
Before that you might  know us

00:00:22.480 --> 00:00:24.599
because of our other projects such as PyMVPA and NeuroDebian.

00:00:25.140 --> 00:00:26.901
I hope that you use them

00:00:26.901 --> 00:00:30.129
and they help you in your research projects.

00:00:30.129 --> 00:00:32.660
More about these and other projects

00:00:32.660 --> 00:00:36.530
You could discover if you go to the centerforopenneuroscience.org website,

00:00:36.530 --> 00:00:37.680
or you could also find

00:00:37.860 --> 00:00:43.100
contacts for us in social media and before I proceed with the talk

00:00:43.110 --> 00:00:47.272
I want first of all acknowledge work of others on the project.

00:00:47.272 --> 00:00:49.650
It wasn't only my and Michael's work

00:00:51.580 --> 00:00:54.201
Our project is heavily based on Git-annex tool,

00:00:54.201 --> 00:00:57.659
which Joey Hess wrote for managing his own collection of files

00:00:58.060 --> 00:01:00.269
which has nothing to do with science.

00:01:01.240 --> 00:01:04.229
Also, he is well known for his work in Debian project

00:01:04.600 --> 00:01:09.390
We had... we still have tireless workers on a project

00:01:09.909 --> 00:01:11.020
Benjamin

00:01:11.020 --> 00:01:14.140
working with Michael and Alex.

00:01:14.140 --> 00:01:16.920
Alex recently refurbished or wrote from scratch a new version of the website

00:01:16.920 --> 00:01:20.249
I hope that you'll like it and we'll see a bit more of it later.

00:01:21.490 --> 00:01:25.439
Also, we had Jason, Debanjum and Gergana working on the project.

00:01:26.469 --> 00:01:30.299
They were quite successful to accomplish a lot within short period of time

00:01:31.119 --> 00:01:33.260
So if you're looking for a project to contribute to

00:01:33.260 --> 00:01:37.199
it might be the interesting project for you to start

00:01:37.740 --> 00:01:39.600
working on open source projects

00:01:39.600 --> 00:01:42.200
and leave in kind of your foot step in the

00:01:42.260 --> 00:01:46.160
ecosystem of Open Source for Neuroscience.

00:01:46.160 --> 00:01:49.920
This project is supported by NSF and

00:01:50.100 --> 00:01:53.960
Federal Finistry for Education and Research in Germany.

00:01:54.400 --> 00:02:00.160
If you go to centerforopenneuroscience.org you could discover more

00:02:00.380 --> 00:02:04.500
interesting and exciting projects we either collaborate with it or contribute to.

00:02:06.369 --> 00:02:12.239
Before we proceed I want actually to formulate the problem we are trying to solve DataLad.

00:02:12.970 --> 00:02:16.506
Data is second class citizen within software platforms.

00:02:16.506 --> 00:02:18.499
What could that potentially be?

00:02:20.310 --> 00:02:25.009
One of the aspects is if you look how people distribute data nowadays

00:02:25.710 --> 00:02:32.239
Quite often you find that even large arrays of data are distributed in tarballs or zip files.

00:02:34.110 --> 00:02:37.459
Problems were multiple with these ways of distribution

00:02:37.459 --> 00:02:40.249
if one file changes you need to re-distribute

00:02:40.820 --> 00:02:42.540
Entire tarball which might be gigabytes in size,

00:02:42.540 --> 00:02:48.120
and that's why partially we also couldn't just adopt

00:02:48.720 --> 00:02:50.130
technologies which are

00:02:50.130 --> 00:02:54.840
proven to work for software, let's say in Debian we distribute complete packages.

00:02:54.980 --> 00:02:56.420
But again the problem is the same.

00:02:56.660 --> 00:02:59.160
As long as you force

00:02:59.360 --> 00:03:04.220
wrapping all the data together in some big file - it wouldn't work. It won't scale.

00:03:06.060 --> 00:03:09.023
Also another problem is absent version of data.

00:03:09.023 --> 00:03:15.060
And many people actually underappreciate it and think that it doesn't actually exist

00:03:15.060 --> 00:03:21.780
or relates to their way of work. But no, actually this problem is quite generic.

00:03:22.920 --> 00:03:24.920
So if you look into this

00:03:25.620 --> 00:03:30.860
PhD comics caricature, you'll find that these probably relates to many

00:03:32.549 --> 00:03:35.209
ways how you deal with files, data or

00:03:35.880 --> 00:03:39.079
documents. And you could see that actually

00:03:39.930 --> 00:03:47.600
how we tend to version our data is by providing - quite often - the date, right, which creates some kind of linear progression.

00:03:48.239 --> 00:03:50.539
Right, so we annotate that: "Oh!"

00:03:51.540 --> 00:03:56.209
"I've worked on these in those dates, but also maybe a little bit later..."

00:03:56.209 --> 00:04:01.129
And we try to annotate it with some description of what was maybe done to the data

00:04:01.380 --> 00:04:04.399
Right, so in this case. It was a test run

00:04:04.400 --> 00:04:09.890
Then we test it again and calibrate it and then we ran into a problem, right? So...

00:04:10.470 --> 00:04:16.519
All these, kind of, you saved the result of your work and annotated so later on you could

00:04:17.010 --> 00:04:23.779
either get back to the previous state. Let's say maybe you indeed made an error and you want to rollback.

00:04:24.419 --> 00:04:29.939
Or maybe you want to just compare what have you done, which broke your code or data?

00:04:30.700 --> 00:04:34.770
Right, and as you could see those messages could be quite descriptive.

00:04:35.830 --> 00:04:43.679
But the problem is that version control systems which are created for code are inadequate for data, right? So the problem is,

00:04:44.200 --> 00:04:51.029
quite often, that it's duplication you have copy of the data in the version control system inside somewhere

00:04:51.030 --> 00:04:52.450
so you couldn't use it directly.

00:04:52.450 --> 00:04:57.440
But also it's present on your hard drive, so at least you have two copies quite often.

00:04:57.600 --> 00:05:02.520
Or maybe it's duplicated and just on a single server, right?

00:05:03.010 --> 00:05:09.330
I could give you examples were data in a version control system filled up the version control system and

00:05:09.940 --> 00:05:12.480
meanwhile filling up the hard drive and

00:05:13.180 --> 00:05:19.770
sometimes you try to commit new file and apparently ran out of space on the server and it might ruin your

00:05:20.110 --> 00:05:22.319
version control back and then on the server

00:05:23.020 --> 00:05:28.409
Rendering it impossible to get to the previous version, so you don't want to have that, right?

00:05:29.830 --> 00:05:37.770
Then another problem is that there are no generic data distributions or at least there were no before DataLad.

00:05:38.110 --> 00:05:40.410
So there is no efficient ways to

00:05:41.170 --> 00:05:43.259
install and upgrade data sets and

00:05:44.740 --> 00:05:52.109
When you also deal with different data hosting portals you need to learn how to navigate them

00:05:52.110 --> 00:05:52.470
All right

00:05:52.470 --> 00:05:53.830
you need to learn how you

00:05:53.830 --> 00:06:00.149
authenticate, which page you need to go to and what to download and how to download it?

00:06:00.340 --> 00:06:03.750
So just to get to that data set. And then, maybe you

00:06:05.230 --> 00:06:08.129
get the announcement that dataset was fixed

00:06:08.130 --> 00:06:14.309
and you need to repeat this over and over again trying to remember how to deal with it. And I'm not talking even if

00:06:14.920 --> 00:06:22.379
the website became much better and sleeker and changed all the ways how it actually deals with downloads from what it did before.

00:06:23.980 --> 00:06:27.029
Another aspect that data is rarely tested

00:06:27.160 --> 00:06:29.999
So what does it mean for data to have bugs?

00:06:30.340 --> 00:06:36.210
Any derived data is a product of running a script or some kind of procedure on

00:06:36.880 --> 00:06:39.839
original data and generating new derived data.

00:06:40.990 --> 00:06:44.939
Quite permanent ones which you could find in references later on in this presentation

00:06:45.460 --> 00:06:52.079
is atlases. So Atlas is usually produced from the data writing some really sophisticated script

00:06:52.330 --> 00:06:53.920
which generates new data:

00:06:53.920 --> 00:06:56.819
the atlas. And those atlases could be buggy.

00:06:57.160 --> 00:07:03.630
So how do you test the data? The same way as software. If we could establish this efficient process where we

00:07:04.630 --> 00:07:08.999
produce some data and verify that at least data meets the assumptions

00:07:09.000 --> 00:07:12.869
which you expect. If it's population or probability in the area

00:07:12.870 --> 00:07:15.990
which must be present in the entirety of population,

00:07:16.060 --> 00:07:21.300
then operability should be up to 100 or nearby 100. If it doesn't add up,

00:07:22.060 --> 00:07:25.020
then you have a bug. It's really simple assumption

00:07:25.020 --> 00:07:28.145
But very verifying that your data doesn't have...

00:07:28.145 --> 00:07:30.779
Doesn't break those is really important.

00:07:32.140 --> 00:07:38.729
Unified way how we deal with data, and the code could help to establish those data testing procedures.

00:07:39.580 --> 00:07:46.619
Also, it's quite difficult to share your derived data. If downloaded some data set from an well known portal...

00:07:46.720 --> 00:07:50.160
How do you share it? What data could be shared?

00:07:51.100 --> 00:07:53.340
Where do you deposit it so people later on

00:07:53.830 --> 00:07:59.819
Could download maybe original data, and your derive data without even worrying that oh

00:07:59.820 --> 00:08:04.710
They need to get this piece from an original source and your derived data from another place

00:08:05.110 --> 00:08:09.119
So how do we link those pieces together to make it convenient?

00:08:10.210 --> 00:08:17.279
What we're trying to achieve is to make managing of the data as easy as managing code and software.

00:08:18.430 --> 00:08:21.690
Is it possible? I hope that you'll see that it is so

00:08:22.300 --> 00:08:23.350
 

00:08:23.350 --> 00:08:25.350
What DataLad is based on...

00:08:25.780 --> 00:08:31.380
Is in two pieces and one of them is Git. I hope that everybody knows what Git is.

00:08:31.900 --> 00:08:33.010
 

00:08:33.010 --> 00:08:38.369
But I'll give small presentation nevertheless. So Git is a version control system

00:08:38.650 --> 00:08:46.259
and initially it was developed to manage Linux project code, if somebody doesn't know what Linux is this is one of the

00:08:46.840 --> 00:08:54.599
most recognized and probably mostly used, because it's used everywhere: on the phones, on the servers, on the operating systems.

00:08:54.970 --> 00:09:02.010
It's free and open source and it's developed into open and at some point they needed new version control system

00:09:02.010 --> 00:09:07.739
Which would scale for the demand of having lots of code managed there and many people working with it

00:09:07.980 --> 00:09:13.360
So it's not a geeky project just for... Between a few people.

00:09:13.900 --> 00:09:16.000
It is developed by hundreds.

00:09:16.000 --> 00:09:17.240
It's used by millions.

00:09:18.540 --> 00:09:21.560
What's great about Git is that it's distributed.

00:09:21.780 --> 00:09:27.440
So content is available across all copies of the repository if you clone the repository

00:09:28.000 --> 00:09:30.960
You have the entire history of the project

00:09:30.960 --> 00:09:35.840
and you could get to any point in that development you could compare different versions.

00:09:35.840 --> 00:09:41.080
You could do exactly the same things as original developers dated on this repository.

00:09:41.080 --> 00:09:46.900
So it provides you as much flexibility to accomplish things locally

00:09:47.470 --> 00:09:50.460
without requiring any network access.

00:09:51.070 --> 00:09:55.679
Git became a backbone for github and other social coding portals.

00:09:55.900 --> 00:10:01.860
So github came to fill the niche that there were no convenient online resource

00:10:01.960 --> 00:10:05.960
where people could easily share these repositories and work on them together.

00:10:06.480 --> 00:10:12.820
So git is just a tool and github is just a web portal which provides you

00:10:13.000 --> 00:10:20.720
a convenient centralized management of the repositories and collaboration between people.

00:10:20.740 --> 00:10:23.220
But it's not a single one there are other

00:10:23.400 --> 00:10:25.740
systems which use Git underneath.

00:10:26.400 --> 00:10:29.740
Gitlab, Bitbucket, so...

00:10:31.000 --> 00:10:37.320
It just creates this the entire ecosystem of the tool and additional services and resources.

00:10:38.280 --> 00:10:48.120
What git is great for is very efficient management of textual information, right, so if you manage code, text, configuration files...

00:10:48.280 --> 00:10:58.700
Maybe dumped some documentation or JSON files? So, all of those were nicely managed by git because it has really good mechanism to

00:10:58.940 --> 00:11:02.662
annotate the differences and compress that efficiently.

00:11:02.662 --> 00:11:07.000
So all those distributed copies are actually not that big.

00:11:07.200 --> 00:11:10.560
But the problem or inefficiency of Git

00:11:10.820 --> 00:11:17.020
is this exactly distributed nature of it. That it stores all the copies of the documents

00:11:17.320 --> 00:11:23.400
on all the systems, right? So, if I have big files, then it becomes inefficient.

00:11:23.400 --> 00:11:28.300
because now you will have two copies, right? You will have one on the hard drive (at least two copies)...

00:11:28.520 --> 00:11:32.049
One on your hard drive and then one committed into Git.

00:11:32.050 --> 00:11:35.620
Then if you push this into Github you will have again a

00:11:35.779 --> 00:11:40.029
big copy of that file somewhere and anybody who clones that repository

00:11:40.580 --> 00:11:43.900
might wait for a while to just get it and then

00:11:44.000 --> 00:11:51.640
they might be a little bit upset because they wanted just one file from the repository and didn't care to download a gigabyte

00:11:52.120 --> 00:11:54.180
of data just to see it.

00:11:54.180 --> 00:11:56.640
So it's inefficient for storing data.

00:11:57.760 --> 00:12:04.980
What is the other tool we rely on, as I said, written by Joey Hess it's Git-annex.

00:12:05.240 --> 00:12:11.080
So the idea was to build on top of git to provide management for the data files

00:12:11.720 --> 00:12:15.279
Without committing those files directly into Git.

00:12:16.520 --> 00:12:22.449
So git-annex allows you to add data files under Git control without

00:12:23.120 --> 00:12:26.770
committing the content of the files into Git.

00:12:27.589 --> 00:12:33.748
While playing with Git-annex and DataLad you might see that files get replaced with the same link.

00:12:33.748 --> 00:12:38.280
So what git-annex commits into Git is actually just symlink

00:12:38.280 --> 00:12:41.780
which points to the file which contains the data.

00:12:42.170 --> 00:12:48.130
This way you can commit really lightweight symlink and keep the data on the hard drive and a single

00:12:48.500 --> 00:12:55.299
copy. Sorry, it's not in Git. And then what git-annex does, it orchestrate the

00:12:56.089 --> 00:12:58.089
management of those files between

00:12:58.279 --> 00:13:03.519
Different clones of the repository or so called other way special nodes.

00:13:03.800 --> 00:13:10.630
But also it provides access to those files if they are let's say uploaded on to some website,

00:13:10.630 --> 00:13:15.760
so you have a URL. You could associate the URL with the file, you could upload it to FTP,

00:13:16.100 --> 00:13:18.820
you could upload it to web server.

00:13:19.550 --> 00:13:25.839
You could even get content through BitTorrent, or you could use Amazon s3 storage as your

00:13:26.510 --> 00:13:31.029
container for the files and it allows for custom extensions.

00:13:31.370 --> 00:13:37.389
Let's say you could upload data to Dropbox, Google Drive, box.com and many, many other

00:13:38.839 --> 00:13:40.958
data hosting provider.

00:13:42.079 --> 00:13:46.929
Git-annex takes care also about avoiding the limitations of those platforms.

00:13:47.480 --> 00:13:54.279
Let's say box.com from public account, it doesn't allow you to have files larger than I believe hundred megabytes.

00:13:54.889 --> 00:14:00.489
Git-annex will chop it up so on the box.com you'll have little pieces

00:14:00.490 --> 00:14:03.820
You will not use them directly from box.com, but then git-annex

00:14:03.820 --> 00:14:09.129
will re-assemble the big file when it gets it onto your hard drive so all those

00:14:09.410 --> 00:14:15.339
Conveniences and in addition encryption, that's if you want to share some sensitive data, and you cannot just upload it

00:14:16.130 --> 00:14:20.079
Unencrypted to the public service all those are provided by git-annex.

00:14:20.839 --> 00:14:27.789
Also additional feature which we don't use in a project is git-annex assistant which is Dropbox like

00:14:28.850 --> 00:14:30.850
Synchronization mechanism you could establish

00:14:31.519 --> 00:14:35.649
synchronization between your Git, git-annex repositories across multiple servers and

00:14:35.990 --> 00:14:42.909
configure them really flexibly so you have that's a backup of on off all the data files on one server and

00:14:42.980 --> 00:14:47.589
Some other server will have only files which it cares about let's say

00:14:47.930 --> 00:14:52.299
Data files another one might have only video files

00:14:53.149 --> 00:14:57.698
another one may be just music files who knows so flexibility is there and

00:14:58.060 --> 00:15:02.400
It's all up to you to configure what you want where.

00:15:02.400 --> 00:15:07.480
In our project we don't use it yet, but we do use it locally for synchronizing

00:15:07.700 --> 00:15:10.020
different git-annex repositories.

00:15:12.380 --> 00:15:15.300
But another problem here, so we have really

00:15:20.540 --> 00:15:21.180
great two tools Git and git-annex, but both of them work on a single repository level.

00:15:21.340 --> 00:15:26.560
So, the work in a git repository you need to go into that directory and

00:15:27.320 --> 00:15:29.500
Accomplish whatever you want to do.

00:15:29.500 --> 00:15:33.400
It kind of doesn't go along well with the notion of distribution

00:15:33.840 --> 00:15:38.760
You don't care where you are you just want to, on your  hard drive, so you just want to say:

00:15:39.100 --> 00:15:42.720
Oh, search, find me something, install this and

00:15:43.140 --> 00:15:46.940
give me access to this data. Right? Or get me give me this file

00:15:47.080 --> 00:15:50.700
Even though maybe I'm not in that git or git-annex repository

00:15:52.180 --> 00:15:56.780
Also another kind of aspect those are just tools  so similarly like

00:15:57.120 --> 00:16:02.820
how GitHub provided convenient portal to the tool git.

00:16:03.760 --> 00:16:07.178
We want to accomplish something where we use these tools

00:16:07.178 --> 00:16:09.620
which are agnostic of domain of the data

00:16:09.630 --> 00:16:11.290
(let's say neuroimaging)

00:16:11.290 --> 00:16:16.410
to give you guys access to those terabytes of publicly shared data already

00:16:16.720 --> 00:16:19.920
which lives out there somewhere,

00:16:19.920 --> 00:16:21.569
so we don't need to collect it. We don't need to

00:16:22.270 --> 00:16:26.309
make copy of it locally, right, it's already there, so

00:16:26.950 --> 00:16:31.890
What we want to achieve is just to provide access to that data without

00:16:32.230 --> 00:16:36.180
Mirroring it on our servers or without duplicating it elsewhere

00:16:38.260 --> 00:16:47.740
Before going into demos I want to give you kind of more illustrative demo of what is data lifecycle here of data

00:16:47.940 --> 00:16:51.140
which we provide by DataLad.

00:16:51.340 --> 00:16:59.180
Let's imagine that we have a data set which comes initially from OpenfMRI, right, and live somewhere in the cloud or

00:16:59.410 --> 00:17:07.139
On data hosting portal actually we have two copies of the data one of them might be in the tarball, somewhere on HTTP server

00:17:07.420 --> 00:17:09.420
right and another one might be

00:17:09.850 --> 00:17:16.860
Extracted from the tarball, somewhere on a cloud which might have HTTP access might have S3 access,

00:17:16.860 --> 00:17:18.900
but the point is that data is there and

00:17:19.480 --> 00:17:25.980
Then we have a data user and that's us right me you everybody who wants to use this data

00:17:26.160 --> 00:17:28.160
So now options are: we either...

00:17:28.390 --> 00:17:34.680
Go down on the tarball extract it or we learn how to use S3 and go and install some tool

00:17:35.440 --> 00:17:37.589
Browse S3 bucket, download those files.

00:17:38.950 --> 00:17:42.510
But what we are trying to establish here is actually a middle layer, right?

00:17:42.710 --> 00:17:48.499
We want to provide data distribution which might be hosted somewhere, maybe it's on github maybe in our server

00:17:49.050 --> 00:17:55.310
Where I'll take this data available online and will automatically crawl it so here

00:17:55.310 --> 00:18:02.600
I mentioned this command crawl which is one of the commands DataLad provides to automate monitoring of external resources

00:18:03.750 --> 00:18:09.230
So we could get them into Git repositories and actually you could see here that these

00:18:11.040 --> 00:18:12.750
Greenish-yellow

00:18:12.750 --> 00:18:15.440
Why you don't draw here? Greenish yellow...

00:18:16.260 --> 00:18:17.550
color.

00:18:17.550 --> 00:18:19.550
Why you don't draw here?

00:18:20.580 --> 00:18:27.919
Here we go! So, this greenish yellow color represents just a Content reference

00:18:28.620 --> 00:18:34.280
Instead of the actual content, that's why we could host it on github or anywhere because it doesn't have the actual data

00:18:35.250 --> 00:18:40.310
So we collect those data sets into collections, which we might share

00:18:40.310 --> 00:18:44.929
let's save the one which we share from data sets that are on DataLad.org

00:18:45.330 --> 00:18:51.080
underneath we use git modules which is built-in mechanism within Git to organize these collections of

00:18:51.270 --> 00:18:55.340
multiple repositories while keeping track of burgeoning information

00:18:55.340 --> 00:18:58.369
So you could get the entire collection of let's say of OpenfMRI data sets

00:18:58.560 --> 00:19:02.749
For a specific date for a specific version if you want to reproduce some of these else analysis

00:19:02.750 --> 00:19:06.140
And then we are making it possible to install

00:19:06.660 --> 00:19:10.009
Arbitrary number of those data sets we are unified interface

00:19:10.710 --> 00:19:16.639
So here we mentioned command datalad --install which you will see later and hopefully

00:19:17.400 --> 00:19:22.400
Those parameters like install into current data set and get all the data

00:19:23.010 --> 00:19:28.550
Will it be less surprising and also we provide shortcuts so which I'll talk about later

00:19:28.800 --> 00:19:31.100
But the point is that you could now easily

00:19:31.680 --> 00:19:36.680
Install those data sets onto your local hard drive, and if you are doing some processing

00:19:37.530 --> 00:19:44.810
It might add results of the process in this case. We've got new file filtered bold file, which we could easily add

00:19:45.660 --> 00:19:50.480
Into this repository and which means which is committed into the repository

00:19:51.000 --> 00:19:58.739
Under git-annex control. And later we could publish this entirety of maybe collection of the datasets

00:20:00.010 --> 00:20:05.010
to multiple places one of them might be github or we publish only the

00:20:06.130 --> 00:20:08.729
repository itself without actually data files again

00:20:08.730 --> 00:20:16.170
Those are just symlinks and maybe offload the actual data to some server which my HTTP server

00:20:18.100 --> 00:20:25.230
or some other server through some mechanism right, but the point is that data goes somewhere and the magic happens here

00:20:25.330 --> 00:20:31.709
Thanks to the git-annex because that's the Beast which keeps track of were each data file

00:20:32.080 --> 00:20:36.929
Could be obtained from so this red links point to the information

00:20:36.930 --> 00:20:44.339
What git-annex stores for us that I let's say this bald file is available from original web portal right it's available from S3 bucket,

00:20:44.340 --> 00:20:48.300
it might be coming from a tarball, so that's one of the extensions

00:20:48.300 --> 00:20:53.190
we added to git-annex to support extraction of the files from the tarball.

00:20:53.740 --> 00:20:57.330
So it becomes really transparent to the user and this new file

00:20:58.570 --> 00:21:04.889
We published it there. So it might be available now through HTTP so people who cloned this repository

00:21:06.220 --> 00:21:13.620
Would be able to get any file from original storage or from any derived data

00:21:13.720 --> 00:21:15.720
Which we published on our website?

00:21:16.840 --> 00:21:20.250
So that's kind of the main idea behind DataLad.

00:21:21.610 --> 00:21:23.260
So, altogether

00:21:23.260 --> 00:21:28.650
DataLad allows you to manage multiple repositories organized into these super datasets

00:21:28.650 --> 00:21:34.680
Which are just collection of git repositories using standard git sub-modules mechanism.

00:21:34.990 --> 00:21:38.760
It supports both git and ggit-annex repository, so if you have

00:21:39.490 --> 00:21:45.180
Just regular git repositories where you don't want to add any data. It's perfectly fine.

00:21:45.940 --> 00:21:52.499
We can crawl external online data resources and update git-annex repositories upon changes.

00:21:53.770 --> 00:21:59.160
It seems to scale quite nicely because data stays with original data provider

00:21:59.160 --> 00:22:02.369
so we don't need to increase the storage in our server and

00:22:02.920 --> 00:22:09.020
We could use maybe or you could use because anybody could use DataLad to publish

00:22:09.200 --> 00:22:11.300
their collections of the datasets on

00:22:12.760 --> 00:22:19.679
github and maybe offloading data itself to portals like box.com or dropbox.

00:22:21.279 --> 00:22:25.769
What happens now that we have unified access to data regardless of its origin

00:22:25.770 --> 00:22:30.389
I didn't care if data comes from openfMRI or CRCNS.

00:22:30.820 --> 00:22:36.960
The only difference might be that you need to authenticate it. Let's say CRCNS doesn't allow download without authentication.

00:22:37.539 --> 00:22:42.929
So DataLad will ask you for credentials, which you should store locally in the hard drive

00:22:42.960 --> 00:22:46.649
Nothing is shared with us and later on when you need to get more data

00:22:46.750 --> 00:22:51.089
Just to use those credentials to authenticate in your behalf to CRCNS,

00:22:51.640 --> 00:22:55.559
download those tarballs extract it for you, so you didn't need to worry about that and

00:22:56.260 --> 00:23:00.929
Also, we take care about serialization, so if original website distributes only tarballs

00:23:01.779 --> 00:23:04.919
We download tarballs for you, extract them and again

00:23:04.919 --> 00:23:08.939
You didn't need to worry how the data is actually serialized by original data provider

00:23:09.940 --> 00:23:13.770
What we do on top is that we aggregate metadata.

00:23:14.320 --> 00:23:18.390
What metadata is? It is data about the data.

00:23:18.880 --> 00:23:24.779
So let's say you have a data set which contains the data the results information about what this data

00:23:24.779 --> 00:23:28.409
Set is about what it's named. What was its offer authors?

00:23:29.080 --> 00:23:31.640
What might be the license if it's applicable?

00:23:32.160 --> 00:23:35.880
so any additional information about the data constitutes metadata.

00:23:36.140 --> 00:23:42.080
What we do in DataLad, we aggregate metadata, which we find about the original data sets and

00:23:42.820 --> 00:23:44.490
Provide you convenient interface

00:23:44.490 --> 00:23:48.329
So you could search across all of it across all the data sets which we already

00:23:48.640 --> 00:23:52.049
Integrated in DataLad. And I hope you'll see the demonstration

00:23:52.750 --> 00:23:54.959
quite appealing later on.

00:23:56.049 --> 00:24:01.679
Then DataLad after you consumed added extended data sets or just created from scratch

00:24:02.380 --> 00:24:09.839
You could share original or derived datasets publicly as I mentioned or internally you could always

00:24:10.659 --> 00:24:16.709
publish them locally at your SSH may be to collaborate with somebody and that's what we do regularly and

00:24:17.919 --> 00:24:19.919
Meanwhile we'll keep data

00:24:20.320 --> 00:24:27.780
we could keep data available elsewhere, or you could even share the data set without sharing the data, which is quite keen as

00:24:29.860 --> 00:24:36.360
Demonstration of good intent when you are about to publish the paper, that's what we did them with our recent submission

00:24:36.360 --> 00:24:40.829
We publish the data set but not with the entirety of the data set

00:24:40.830 --> 00:24:45.449
But just with first subject so reviewers could verify that there is

00:24:46.450 --> 00:24:48.130
Good quality data

00:24:48.130 --> 00:24:49.990
that

00:24:49.990 --> 00:24:56.099
They could get access to it right and that the entirety of data is in principle available

00:24:56.100 --> 00:25:00.089
And it was processed accordingly because the whole the entire Git history

00:25:00.850 --> 00:25:04.650
is maintained and shared, but the data files are not

00:25:06.160 --> 00:25:10.560
Okay and the additional benefit some of it, which is work in progress

00:25:11.080 --> 00:25:15.449
You could export the data set if you want to share just the data itself you could

00:25:15.580 --> 00:25:21.420
Export the data set and current version in a tarball and give it to somebody but more exciting feature

00:25:21.700 --> 00:25:25.680
and we've been working on is exporting in to

00:25:26.290 --> 00:25:27.370
some

00:25:27.370 --> 00:25:31.170
Metadata heavy data formats if you're publishing scientific data

00:25:31.660 --> 00:25:37.170
You will be asked to fill out a big spreadsheet, which is called easy to have

00:25:38.950 --> 00:25:44.340
To annotate metadata for your data set it's really tedious and unpleasant job

00:25:44.340 --> 00:25:48.630
But the beauty is that all that information is contained within

00:25:49.030 --> 00:25:54.180
metadata of either data set or of git-annex. So we could automatically

00:25:54.580 --> 00:26:01.199
export majority of information for you, so you just need to fill out left out information and be done

00:26:04.150 --> 00:26:07.680
DataLad comes with both common line and Python interfaces

00:26:07.680 --> 00:26:14.489
So you could work with it interactively either in common line or script it in bash or working with it interactively in the ipython

00:26:14.830 --> 00:26:20.100
and script it with Python language it gives you the same capabilities and really similar syntax

00:26:22.300 --> 00:26:24.100
Our distribution

00:26:24.100 --> 00:26:27.089
Grew up already to cover over ten terabytes of data

00:26:27.910 --> 00:26:31.469
We cover such data sets as OpenfMRI, CRCNS,

00:26:32.560 --> 00:26:34.560
functional connectome

00:26:34.870 --> 00:26:42.430
INDI data sets and even some data sets from Kaggle and some RatHole radio podcast show

00:26:42.830 --> 00:26:49.059
Because it was a cool experiment to be able to crawl that website and collect all the data

00:26:49.520 --> 00:26:52.599
About timing of the songs. So check it out

00:26:52.600 --> 00:26:57.100
It's available on github although data stays as again with original provider

00:26:57.370 --> 00:27:03.609
What is coming? More data, so we'll cover human connectome project and data available from x net servers

00:27:03.890 --> 00:27:08.410
We want to provide extended metadata support, so we cover not only data sets

00:27:08.410 --> 00:27:09.190
level data

00:27:09.190 --> 00:27:16.750
But also data for separate files if you know about any other interesting data set or data provider

00:27:17.390 --> 00:27:20.739
File a new issue, or shoot us an email.

00:27:21.590 --> 00:27:27.850
we are also working on integrating with NeuroDebian, so you could apt-get install those datasets and the position of data to

00:27:28.580 --> 00:27:32.350
OSF and in other platforms. Another interesting integration

00:27:32.350 --> 00:27:39.880
Which we've done was to introduce DataLad support into HeuDiConv which stands for Heuristic DICOM Conversion Tool.

00:27:39.880 --> 00:27:41.809
which allows you to

00:27:41.809 --> 00:27:47.739
automate conversion of your DICOM data obtained from MRI scanner into NIfTI files

00:27:48.080 --> 00:27:51.520
but we went one step further and

00:27:52.280 --> 00:27:57.489
Standardized it to convert not only to DataLad data set but DataLad BIDS data sets.

00:27:57.490 --> 00:28:02.020
Set so if you don't know what BIDS is, it is something you must know nowadays.

00:28:02.540 --> 00:28:07.479
If you doing imaging research. It's brain imaging data structure format

00:28:07.700 --> 00:28:14.679
Which describes how you should lay out your files on a file system so anybody who finds your data set will be immediately

00:28:15.230 --> 00:28:17.230
capable to understand

00:28:17.690 --> 00:28:24.970
your design how many subjects you have so it's standardized is beyond NIfTI. It standardized is how you

00:28:25.280 --> 00:28:27.280
Work with your files so now

00:28:27.680 --> 00:28:33.500
With this integration HeuDiConv we can obtain DataLad datasets

00:28:34.360 --> 00:28:39.660
with BIDS if I'd neuroimaging data, so it's ready to be shared

00:28:39.670 --> 00:28:44.109
It's ready to be processed by any BIDS compatible tool, so it opens ample

00:28:44.660 --> 00:28:46.660
opportunities

00:28:46.790 --> 00:28:50.930
And at this point I guess we should switch and do some demos

00:28:53.640 --> 00:28:59.989
And before I actually give any demo I want to familiarize you with our new website DataLad.org

00:29:01.110 --> 00:29:03.000
On top you could see

00:29:03.000 --> 00:29:06.619
navigation for among major portions the website

00:29:07.140 --> 00:29:09.170
One of them is about page

00:29:09.870 --> 00:29:13.910
Just describes the purpose of the DataLad and provides

00:29:14.580 --> 00:29:18.379
information about funding agencies and involved institutions

00:29:20.010 --> 00:29:22.010
Next link is "Get DataLad"

00:29:22.890 --> 00:29:30.350
Which describes how to install the DataLad. The easiest installation is if you are using your Debian already.

00:29:30.600 --> 00:29:38.280
Then it just apt-get install DataLad command or you could find it in package manager and install it within second

00:29:38.720 --> 00:29:45.320
Alternatively, if you are on OS-X or any other operating system. Windows support is initial but it

00:29:46.230 --> 00:29:49.190
Should work in the basic set of features

00:29:49.800 --> 00:29:51.950
you have to install git-annex by

00:29:52.500 --> 00:29:54.590
going to git-annex website and

00:29:56.850 --> 00:29:58.850
Into install page

00:29:59.280 --> 00:30:05.840
choosing the operating system of your choice and following the instructions there how to get it and

00:30:07.200 --> 00:30:09.140
after you installed git-annex

00:30:09.140 --> 00:30:15.499
You just need to install DataLad from Python package index through pip-install datalad command.

00:30:16.920 --> 00:30:19.399
Next page is features page

00:30:19.410 --> 00:30:27.379
Which is actually led to by those pretty boxes on the main page and this page will go through

00:30:27.840 --> 00:30:29.840
later in greater detail

00:30:30.150 --> 00:30:33.379
Another interesting page is Datasets which presents you our

00:30:34.110 --> 00:30:39.200
ultimate official distribution which points to datasets.datalad.org

00:30:39.990 --> 00:30:44.479
which is the collection of data sets which already pre crawled for you and

00:30:45.240 --> 00:30:50.420
That were we provide those data sets like for Open fRI

00:30:51.510 --> 00:30:53.340
CRCNS

00:30:53.340 --> 00:30:55.290
ADHD and

00:30:55.290 --> 00:30:56.940
many others

00:30:56.940 --> 00:31:00.469
I will just briefly describe the features of these

00:31:00.990 --> 00:31:07.819
Basic website and mention that the such websites if you have any HTTP server available somewhere

00:31:08.130 --> 00:31:13.459
Maybe institution provides because you will not host the data actually here, or you don't have to

00:31:14.429 --> 00:31:21.349
you could upload similar views of your data sets pretty much anywhere where you could host a website and

00:31:22.140 --> 00:31:26.479
OpenfMRI, let's say we go to OpenfMRI, it lists all those data sets

00:31:26.480 --> 00:31:31.459
which we crawled from OpenfMRI, you could see also immediately mentioning of the version

00:31:31.980 --> 00:31:33.980
here and version goes

00:31:33.990 --> 00:31:35.990
from

00:31:36.000 --> 00:31:42.949
What version OpenfMRI gave it but also with additional indices pointing to exact commits

00:31:44.490 --> 00:31:50.539
Within our git repository I didn't find that version another neat feature here is

00:31:51.389 --> 00:31:52.919
immediate

00:31:52.919 --> 00:31:57.469
Search so you could start typing and now if you're interested in resting-state

00:31:58.320 --> 00:32:00.320
So here we go it goes

00:32:01.529 --> 00:32:06.199
Pretty fast and limits the view only the data sets where metadata

00:32:07.350 --> 00:32:12.169
Mentions this word and say let's look for Haxby... There we go!

00:32:12.779 --> 00:32:18.619
Or let's look for "movie". There we go! So, you could quickly identify the data sets by

00:32:19.350 --> 00:32:21.829
browsing and we'll see how we could do

00:32:22.289 --> 00:32:25.039
such actions later in the command line and

00:32:25.289 --> 00:32:31.638
When you get to the data set of interest or it could be at any pretty much level you'll see on top the command which

00:32:31.639 --> 00:32:35.509
Could be used to install this data set and described in some options

00:32:35.510 --> 00:32:40.969
Let's say -r is to install this data set with any possible sub data set recursively.

00:32:41.309 --> 00:32:46.699
There's -g to install it and also obtain all the data for it, and if you want to speed up the

00:32:48.360 --> 00:32:51.110
obtaining the data you could use -J

00:32:51.110 --> 00:32:56.899
And specify the number of parallel downloads your server and bandwidth could allow you

00:32:57.929 --> 00:33:01.788
Okay, let's go back to DataLad website and another

00:33:03.779 --> 00:33:10.489
Page on the website is development. So, if you're interested to help and contribute datasets provide

00:33:11.039 --> 00:33:13.039
patches improve documentation

00:33:13.320 --> 00:33:20.140
All of the development this is made in open. We use github intensively, we use Travis, we use codecov.

00:33:20.920 --> 00:33:27.200
We use Grid for documentation so and that will be our next point

00:33:28.509 --> 00:33:33.479
Documentation is hosted on docs.datalad.org and it provides

00:33:34.720 --> 00:33:39.480
Not yet as thorough documentation as we wanted but some

00:33:39.789 --> 00:33:46.289
documentation about major features of the dataset or a comparison between Git, git-annex and DataLad.

00:33:46.659 --> 00:33:48.659
But it also provides

00:33:49.179 --> 00:33:56.429
Really thorough interface documentation so as I mentioned before we have command line and Python

00:33:57.519 --> 00:34:01.199
interfaces both of those interfaces generated from the same code

00:34:01.200 --> 00:34:03.539
So they should be pretty much identical

00:34:03.759 --> 00:34:07.829
It just depending how you use command line or Python it will be different

00:34:07.839 --> 00:34:11.129
But otherwise all the options all the commands

00:34:11.169 --> 00:34:15.449
They look exactly the same and in command line reference

00:34:15.450 --> 00:34:23.069
You could find all the documentation for all the commands you could use it that I have some popular ones in my case

00:34:23.069 --> 00:34:29.099
Right where I went before and it provides documentation what those and of course there is

00:34:30.129 --> 00:34:33.629
notes for power users and quite elaborate

00:34:34.179 --> 00:34:37.648
documentation here about all the options which are available

00:34:38.230 --> 00:34:40.230
in those commands

00:34:41.139 --> 00:34:45.779
Ok so let's go back to features and

00:34:47.710 --> 00:34:52.859
First of the demos which I want to show you will be about data discovery

00:34:54.399 --> 00:34:59.608
That's any other demo on the website and is provided with

00:35:00.880 --> 00:35:03.420
screencast which shows all

00:35:04.720 --> 00:35:07.770
necessary commands to carry out the

00:35:09.670 --> 00:35:12.119
Presentation, but also provides you with

00:35:13.210 --> 00:35:17.399
comments describing the purpose of the actions taken

00:35:18.520 --> 00:35:20.079
moreover

00:35:20.079 --> 00:35:25.989
You could obtain the full script for the demo so you could run it as case on your hardware

00:35:28.040 --> 00:35:32.379
By clicking underneath the screen screencast but

00:35:33.800 --> 00:35:38.769
For this demonstration. I'll do it interactively in a shell together with you

00:35:40.280 --> 00:35:41.960
So

00:35:41.960 --> 00:35:43.960
Let's get started!

00:35:44.510 --> 00:35:46.540
If as you remember

00:35:47.270 --> 00:35:53.590
We aggregate a lot of metadata in DataLad to provide efficient search mechanisms

00:35:55.520 --> 00:35:59.919
In this example we'll imagine that we were looking for a data set which mentions

00:36:00.650 --> 00:36:07.600
Raiders in his word after being associated with movie Raiders of the Lost Ark and during imaging

00:36:09.230 --> 00:36:12.399
So we'll use datalad -search command where we'll

00:36:13.430 --> 00:36:16.300
Just state it right, so we'll call datalad -search

00:36:16.300 --> 00:36:22.449
Raiders neuroimaging as with a mini or all commands in DataLad

00:36:23.060 --> 00:36:26.110
They are composed by calling datalad

00:36:26.110 --> 00:36:33.819
then typing the command you want to implement right and then you could ask for help for that command

00:36:36.440 --> 00:36:37.850
Which

00:36:37.850 --> 00:36:39.850
provides you with

00:36:39.950 --> 00:36:41.950
associated help and

00:36:42.170 --> 00:36:47.740
On my screen took a little bit longer just because of video recording usually it's a little bit faster like

00:36:48.380 --> 00:36:50.359
five times and

00:36:50.359 --> 00:36:54.369
Then you actually type the parameters for this command. For search

00:36:54.369 --> 00:36:58.869
It's actually search terms, and I'll present a few other options later on

00:36:59.780 --> 00:37:01.780
whenever you

00:37:02.270 --> 00:37:09.759
Start this command for the first time it will ask you to install our super data set

00:37:11.210 --> 00:37:17.590
Under your home DataLad, in my case that slash demo is the home directory so it asks either

00:37:17.590 --> 00:37:21.609
We want to install that super dates which you saw available on DataLad.org

00:37:22.310 --> 00:37:24.459
in your home directory.

00:37:24.460 --> 00:37:32.139
And that's what it's doing so it quickly installed it because it's just a small git repository without any of those data sets

00:37:32.720 --> 00:37:38.970
Directly in a part of it, but they are linked to it as sub modules. It was really fast, and then it loads and caches

00:37:39.610 --> 00:37:44.069
metadata, which became available in that dataset and that takes few seconds

00:37:51.010 --> 00:37:58.649
Whenever that is done it you see that by default it just returns the paths or names of the

00:38:00.190 --> 00:38:05.369
Datasets as they are within the hierarchy of our super dataset and

00:38:07.510 --> 00:38:09.510
Search searches within the

00:38:10.030 --> 00:38:13.709
Repository data set you are in so if next time

00:38:13.710 --> 00:38:20.159
I am just running the same command it will ask me instead of: "Oh, do you want to install it?" it'll ask me either

00:38:20.160 --> 00:38:23.879
I want to search in this super dataset which I installed in my home directory

00:38:26.530 --> 00:38:28.530
Type yes

00:38:32.710 --> 00:38:37.619
And it provides the same result so to avoid such interactive questions

00:38:37.620 --> 00:38:42.780
you could explicitly mention which data set you want to search in.

00:38:43.060 --> 00:38:45.840
In our case it will be, I'll just specify

00:38:46.570 --> 00:38:49.170
That data set will be this

00:38:49.930 --> 00:38:51.400
canonical

00:38:51.400 --> 00:38:54.270
DataLad data set which is installed in your

00:38:55.210 --> 00:38:57.750
DataLad directory when you specify it like this

00:38:57.750 --> 00:39:06.160
It assumes location in your home directory when you use triple slashes resource identifier as the source for URLs

00:39:06.360 --> 00:39:14.020
To install data sets then it will go to the datasets.datalad.org. And this time we'll search not for Raiders

00:39:14.040 --> 00:39:20.190
neuroimaging, but we'll search for Haxby, one of the authors within this data set

00:39:20.950 --> 00:39:27.839
So -s stands for the fields which we want to search through and -R will report now

00:39:27.840 --> 00:39:30.299
Not just the path to the data set but also

00:39:30.940 --> 00:39:32.940
list the fields which match the

00:39:33.610 --> 00:39:39.120
Query which we ran. So in this case it should search for data sets and report the field "author".

00:39:40.210 --> 00:39:43.919
And only the data sets where Haxby was one of the authors.

00:39:44.620 --> 00:39:46.390
So here they are

00:39:46.390 --> 00:39:49.410
For convenience, let's just switch to that directory

00:39:50.160 --> 00:39:52.160
under our home

00:39:52.329 --> 00:39:56.939
Let me clear the screen and go to that directory

00:39:58.000 --> 00:40:00.299
So now we don't have to specify

00:40:01.539 --> 00:40:03.660
Location of the data set explicitly,

00:40:03.660 --> 00:40:07.360
and we could just type the same query without -d

00:40:07.360 --> 00:40:08.669
and it will provide the same results

00:40:13.420 --> 00:40:15.160
Instead of listing all matching fields,

00:40:15.160 --> 00:40:19.200
let's say in our case it was "author" field, we could

00:40:20.019 --> 00:40:25.409
explicitly specify which fields you want to search through or to report.

00:40:26.289 --> 00:40:27.813
So in this case, I want to see

00:40:27.813 --> 00:40:31.319
what's the name of the dataset and what is the author of the dataset?

00:40:31.319 --> 00:40:33.989
It's already the author, but with didn't see the name.

00:40:35.109 --> 00:40:39.929
And you're on the command to get the output with those fields included

00:40:41.710 --> 00:40:43.630
Well enough of searching!

00:40:43.630 --> 00:40:47.369
Let's clear the screen and what we could do now -- we found

00:40:47.680 --> 00:40:51.539
the datasets right it seems to be that the list of data sets which we found

00:40:52.360 --> 00:40:55.860
is good to be installed and we could just

00:40:56.760 --> 00:40:59.350
rely on a paradigm of Linux

00:40:59.350 --> 00:41:05.680
where you compose commands together through by using pipe command.

00:41:05.860 --> 00:41:08.900
So, what this magic would do?

00:41:08.900 --> 00:41:16.560
If we didn't have these which already what happens -- we get only the list of data sets or past

00:41:16.690 --> 00:41:22.560
Those which are not installed yet,

00:41:22.560 --> 00:41:24.569
and OpenfMRI directory is still empty so we get the list of data sets

00:41:25.320 --> 00:41:29.060
But then instead of manually going and doing:

00:41:29.060 --> 00:41:33.800
"datalad install openfmri/ds00233"...

00:41:33.980 --> 00:41:39.060
or doing copy-paste, we could just say that result of this command

00:41:39.400 --> 00:41:41.400
should be passed as

00:41:41.400 --> 00:41:45.760
Arguments to the next command which will be "datalad install".

00:41:45.760 --> 00:41:48.680
"datalad install" command installs those datasets

00:41:48.680 --> 00:41:51.140
which are either specified by the

00:41:51.819 --> 00:41:56.069
path within current data set or you could provide URLs to

00:41:56.800 --> 00:42:02.300
Install command and it will go to those websites and download them explicitly from there.

00:42:02.300 --> 00:42:05.780
"datalad install" could be used with other resources

00:42:06.340 --> 00:42:10.800
beyond our canonical DataLad distribution.

00:42:11.140 --> 00:42:13.140
So let's run this command

00:42:14.960 --> 00:42:18.640
as a result of it you'll see that now it goes online and

00:42:19.550 --> 00:42:25.630
Installs all those data sets or git/git-annex repositories without any data yet

00:42:25.670 --> 00:42:29.950
So only the files which are committed directly into git will be present.

00:42:42.040 --> 00:42:47.320
And now we could explore what actually we have got here.

00:42:47.320 --> 00:42:52.200
I'll use another DataLad command. Let me clear the screen to bring it on top of the screen.

00:42:53.079 --> 00:42:56.099
Next command is "ls", which just lists

00:42:56.940 --> 00:43:01.400
either data sets or it could be used also at list S3 URLs.

00:43:01.400 --> 00:43:04.380
If you are interested to see what is available in S3 bucket.

00:43:04.380 --> 00:43:11.159
And we are specifying the options: capital -L for long listing, and -r recursively

00:43:11.160 --> 00:43:15.780
So it will go through all data sets locally in current directory.

00:43:15.780 --> 00:43:20.060
(That's why there is a period). And then we'll just remove a list in our data sets

00:43:20.069 --> 00:43:23.129
which are not installed because they are not of our interest here.

00:43:56.050 --> 00:44:01.889
As you can see all those datasets, which we initially searched for and found

00:44:03.820 --> 00:44:05.820
Right?

00:44:09.850 --> 00:44:13.860
They became installed, so they became available on our

00:44:14.500 --> 00:44:19.290
Local file system and "ls" gives us idea. What kind of repository it is

00:44:19.530 --> 00:44:21.870
It's Git versus annex, which branch it is in...

00:44:22.320 --> 00:44:24.560
What was the date of the last commit?

00:44:24.920 --> 00:44:33.360
Also, the sizes what it tells here that we have a lot of 4 gigabytes of data referenced in this data set at the current

00:44:33.900 --> 00:44:39.340
version we've got only 0 bytes locally installed.

00:44:39.520 --> 00:44:44.720
We installed only those symlinks I was talking about.

00:44:46.150 --> 00:44:51.570
So, now we could actually explore what have you got?

00:44:53.290 --> 00:44:58.709
Some of the files that were committed directly into Git, so they became available on the file system as is

00:44:59.320 --> 00:45:06.029
But data files we could obtain now using the "datalad get" command.

00:45:06.670 --> 00:45:09.629
So what this command will do... Let me clear the screen again...

00:45:10.440 --> 00:45:16.960
So you're saying: "Obtain those files! Do it in four parallel processes."

00:45:16.960 --> 00:45:24.040
All the files which match these Shell globe expressions,

00:45:24.040 --> 00:45:26.480
so all the data sets locally which we have

00:45:26.860 --> 00:45:32.759
For all the subjects underneath and anatomical directory, right? We obtained all ready two OpenfMRI dataset

00:45:32.760 --> 00:45:35.040
And now we just want to obtain those data files

00:45:35.320 --> 00:45:42.780
Let's actually see what this one is pointing to... It points to all those data files.

00:45:42.940 --> 00:45:46.240
And if only listed with long listing,

00:45:46.240 --> 00:45:51.480
we'll see that those were symlinks which are actually at the moment not even present on that

00:45:51.820 --> 00:45:56.759
Point into the files which we don't have locally and that's what git-annex would do for us

00:45:56.760 --> 00:45:59.760
It would go online and fetch all those files

00:46:00.910 --> 00:46:02.910
from wherever they are available

00:46:03.520 --> 00:46:05.520
So let me run this command now

00:46:18.220 --> 00:46:21.629
As you can see the are four processes going on

00:46:27.640 --> 00:46:29.640
And the end.

00:46:30.120 --> 00:46:37.060
All DataLad commands they provide you a summary of what actions did it take?

00:46:37.280 --> 00:46:42.020
Here you could see that it got all those files ready to get okay

00:46:42.299 --> 00:46:49.619
Or it might say get failed if it failed to get them and then provides action summary, which we might see later in other demos

00:46:50.650 --> 00:46:57.150
So let's now run the same command which you ran before to see how much of data we actually got?

00:47:26.540 --> 00:47:30.199
As you can see all those which we didn't ask for any data

00:47:30.200 --> 00:47:33.500
They still keep zero bytes although all

00:47:33.810 --> 00:47:37.969
the files that are available and we could browse them,

00:47:37.970 --> 00:47:42.830
but those where we requested additional data files to be obtained finally list how much data

00:47:42.830 --> 00:47:46.159
we have in the working tree

00:47:47.070 --> 00:47:50.059
of those data sets.

00:47:51.120 --> 00:47:54.739
That would complete the demo for "search" and "install".

00:47:56.610 --> 00:48:01.489
Now it's your turn to find some interesting for you data sets and get the data for them

00:48:03.330 --> 00:48:10.009
Now that we went through one of the demos on our website or we call it features which was data discovery

00:48:10.410 --> 00:48:12.860
You could go and visit other

00:48:15.450 --> 00:48:19.010
features described on this page. First one is for data consumers

00:48:19.010 --> 00:48:25.580
which describes how you could generate native DataLad datasets from the website or

00:48:26.250 --> 00:48:29.540
S3 buckets using our crawler so

00:48:30.870 --> 00:48:33.409
If you know some resource you could create your own

00:48:33.870 --> 00:48:40.999
DataLad crawler to obtain that data into DataLad dataset and keep it up to date with periodic reruns.

00:48:41.760 --> 00:48:43.760
Data sharing demo will later show

00:48:44.970 --> 00:48:50.570
examples of how you could share the data either on Github, through the github while depositing data to your website,

00:48:51.090 --> 00:48:56.840
how I demonstrated earlier, or just for collaboration through SSH servers.

00:48:57.870 --> 00:48:59.600
For Git and git-annex users

00:48:59.600 --> 00:49:07.579
We give a little example of unique features present in DataLad contrasting it with

00:49:08.310 --> 00:49:10.881
regular Git and git-annex usage.

00:49:10.881 --> 00:49:16.280
This table outlines there those features.

00:49:16.280 --> 00:49:23.740
We operate on multiple data sets at the same time, we operate across data sets seamlessly

00:49:23.750 --> 00:49:30.169
So you don't have to switch directories to just operate in with specific data files they provide metadata support

00:49:30.860 --> 00:49:37.840
And aggregate from different panel data sources and in unified authentication interface.

00:49:38.480 --> 00:49:40.480
Also, one of the

00:49:40.700 --> 00:49:48.159
new unique features in DataLad is ability to rerun previously ran commands on the data to see how

00:49:49.820 --> 00:49:52.299
things changed or just keep nice

00:49:53.060 --> 00:49:58.509
Protocol of actions you have done and record them within your git/git-annex history.

00:50:00.350 --> 00:50:07.539
And the last one goes in detail in example on how to use HeuDiCon with your data sets and

00:50:07.730 --> 00:50:09.730
relying on our

00:50:10.100 --> 00:50:14.559
naming convention for how to name scanning sequences in the scanner.

00:50:15.640 --> 00:50:18.540
I hope that you liked this presentation

00:50:18.540 --> 00:50:27.700
and you liked what DataLad has to offer so I just want to summarize what DataLad does.

00:50:27.700 --> 00:50:30.300
And what it does? It helps to manage and share

00:50:30.380 --> 00:50:34.300
Available and your own data by a simple command line of Python interface.

00:50:34.880 --> 00:50:38.530
We provide already access to over 10 terabytes of neuroimaging data

00:50:38.530 --> 00:50:46.269
And we help with authentication, crawling of the websites, getting data from the archives in which it was originally distributed

00:50:47.000 --> 00:50:49.000
publishing new or derived data.

00:50:50.000 --> 00:50:55.449
Underneath we use regular pure Git and git-annex repository so whatever tools

00:50:55.450 --> 00:50:58.780
You've got used to use you could still use them

00:50:58.780 --> 00:51:01.810
And if you're an expert git and git-annex user

00:51:02.210 --> 00:51:03.880
We will not limit your powers

00:51:03.880 --> 00:51:11.800
You could do the same stuff what you did before with your key tanga tanga suppositories, so we also provide somewhat human

00:51:12.380 --> 00:51:14.360
accessible

00:51:14.360 --> 00:51:22.060
Metadata interface so in general if you want just to search for some datasets, it's quite convenient with datalad -search.

00:51:23.240 --> 00:51:25.070
Documentation is growing

00:51:25.070 --> 00:51:28.389
You're welcome to contribute, the project is open source.

00:51:29.240 --> 00:51:36.790
I hope that after you've seen the presentation you will agree that managing data can be as simple as manage encode and software. Thank you!

