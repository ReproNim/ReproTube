WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.870 align:start position:0%
 
[Music]

00:00:04.870 --> 00:00:04.880 align:start position:0%
 
 

00:00:04.880 --> 00:00:06.950 align:start position:0%
 
once<00:00:05.200><c> upon</c><00:00:05.440><c> a</c><00:00:05.520><c> time</c><00:00:05.920><c> there</c><00:00:06.160><c> was</c><00:00:06.319><c> an</c><00:00:06.480><c> institute</c>

00:00:06.950 --> 00:00:06.960 align:start position:0%
once upon a time there was an institute
 

00:00:06.960 --> 00:00:08.710 align:start position:0%
once upon a time there was an institute
with<00:00:07.120><c> access</c><00:00:07.520><c> to</c><00:00:07.680><c> a</c><00:00:07.759><c> variety</c><00:00:08.240><c> of</c><00:00:08.400><c> large</c>

00:00:08.710 --> 00:00:08.720 align:start position:0%
with access to a variety of large
 

00:00:08.720 --> 00:00:10.629 align:start position:0%
with access to a variety of large
neuroscientific<00:00:09.440><c> data</c><00:00:09.760><c> sets</c>

00:00:10.629 --> 00:00:10.639 align:start position:0%
neuroscientific data sets
 

00:00:10.639 --> 00:00:12.390 align:start position:0%
neuroscientific data sets
dozens<00:00:11.040><c> of</c><00:00:11.120><c> researchers</c><00:00:11.679><c> depended</c><00:00:12.240><c> on</c>

00:00:12.390 --> 00:00:12.400 align:start position:0%
dozens of researchers depended on
 

00:00:12.400 --> 00:00:14.150 align:start position:0%
dozens of researchers depended on
pre-processed<00:00:12.960><c> versions</c><00:00:13.360><c> of</c><00:00:13.440><c> these</c><00:00:13.679><c> datasets</c>

00:00:14.150 --> 00:00:14.160 align:start position:0%
pre-processed versions of these datasets
 

00:00:14.160 --> 00:00:15.589 align:start position:0%
pre-processed versions of these datasets
for<00:00:14.320><c> the</c><00:00:14.480><c> project</c>

00:00:15.589 --> 00:00:15.599 align:start position:0%
for the project
 

00:00:15.599 --> 00:00:17.430 align:start position:0%
for the project
the<00:00:15.679><c> central</c><00:00:16.080><c> coordinated</c><00:00:16.800><c> pre-processing</c>

00:00:17.430 --> 00:00:17.440 align:start position:0%
the central coordinated pre-processing
 

00:00:17.440 --> 00:00:19.590 align:start position:0%
the central coordinated pre-processing
efforts<00:00:17.840><c> however</c><00:00:18.640><c> suffered</c><00:00:19.039><c> from</c><00:00:19.199><c> a</c><00:00:19.279><c> lack</c><00:00:19.520><c> of</c>

00:00:19.590 --> 00:00:19.600 align:start position:0%
efforts however suffered from a lack of
 

00:00:19.600 --> 00:00:21.750 align:start position:0%
efforts however suffered from a lack of
transparency<00:00:20.240><c> and</c><00:00:20.320><c> reproducibility</c><00:00:21.600><c> there</c>

00:00:21.750 --> 00:00:21.760 align:start position:0%
transparency and reproducibility there
 

00:00:21.760 --> 00:00:24.070 align:start position:0%
transparency and reproducibility there
was<00:00:22.000><c> pre-processed</c><00:00:22.640><c> data</c><00:00:23.119><c> but</c><00:00:23.279><c> over</c><00:00:23.600><c> time</c><00:00:23.920><c> the</c>

00:00:24.070 --> 00:00:24.080 align:start position:0%
was pre-processed data but over time the
 

00:00:24.080 --> 00:00:26.070 align:start position:0%
was pre-processed data but over time the
knowledge<00:00:24.400><c> of</c><00:00:24.560><c> who</c><00:00:24.800><c> created</c><00:00:25.279><c> it</c><00:00:25.519><c> how</c><00:00:25.760><c> it</c><00:00:25.840><c> was</c>

00:00:26.070 --> 00:00:26.080 align:start position:0%
knowledge of who created it how it was
 

00:00:26.080 --> 00:00:28.790 align:start position:0%
knowledge of who created it how it was
created<00:00:26.720><c> or</c><00:00:26.880><c> where</c><00:00:27.119><c> it</c><00:00:27.199><c> was</c><00:00:27.359><c> stored</c><00:00:27.840><c> was</c><00:00:28.080><c> lost</c>

00:00:28.790 --> 00:00:28.800 align:start position:0%
created or where it was stored was lost
 

00:00:28.800 --> 00:00:31.189 align:start position:0%
created or where it was stored was lost
with<00:00:28.960><c> this</c><00:00:29.199><c> lack</c><00:00:29.439><c> of</c><00:00:29.599><c> transparency</c><00:00:30.640><c> reuse</c><00:00:31.039><c> was</c>

00:00:31.189 --> 00:00:31.199 align:start position:0%
with this lack of transparency reuse was
 

00:00:31.199 --> 00:00:33.510 align:start position:0%
with this lack of transparency reuse was
difficult<00:00:32.079><c> and</c><00:00:32.399><c> ceased</c>

00:00:33.510 --> 00:00:33.520 align:start position:0%
difficult and ceased
 

00:00:33.520 --> 00:00:34.870 align:start position:0%
difficult and ceased
but<00:00:33.680><c> when</c><00:00:33.920><c> every</c><00:00:34.239><c> research</c><00:00:34.640><c> group</c>

00:00:34.870 --> 00:00:34.880 align:start position:0%
but when every research group
 

00:00:34.880 --> 00:00:37.030 align:start position:0%
but when every research group
pre-processed<00:00:35.520><c> the</c><00:00:35.680><c> data</c><00:00:36.000><c> individually</c><00:00:36.880><c> it</c>

00:00:37.030 --> 00:00:37.040 align:start position:0%
pre-processed the data individually it
 

00:00:37.040 --> 00:00:38.869 align:start position:0%
pre-processed the data individually it
resulted<00:00:37.440><c> not</c><00:00:37.680><c> only</c><00:00:37.920><c> in</c><00:00:38.079><c> unsustainable</c>

00:00:38.869 --> 00:00:38.879 align:start position:0%
resulted not only in unsustainable
 

00:00:38.879 --> 00:00:41.110 align:start position:0%
resulted not only in unsustainable
duplicate<00:00:39.360><c> computing</c><00:00:39.840><c> efforts</c><00:00:40.480><c> but</c><00:00:40.719><c> also</c>

00:00:41.110 --> 00:00:41.120 align:start position:0%
duplicate computing efforts but also
 

00:00:41.120 --> 00:00:42.869 align:start position:0%
duplicate computing efforts but also
filled<00:00:41.440><c> up</c><00:00:41.520><c> the</c><00:00:41.600><c> disk</c><00:00:41.920><c> space</c><00:00:42.320><c> of</c><00:00:42.399><c> the</c><00:00:42.480><c> compute</c>

00:00:42.869 --> 00:00:42.879 align:start position:0%
filled up the disk space of the compute
 

00:00:42.879 --> 00:00:44.389 align:start position:0%
filled up the disk space of the compute
cluster<00:00:43.360><c> in</c><00:00:43.440><c> no</c><00:00:43.680><c> time</c>

00:00:44.389 --> 00:00:44.399 align:start position:0%
cluster in no time
 

00:00:44.399 --> 00:00:46.869 align:start position:0%
cluster in no time
and<00:00:44.559><c> when</c><00:00:44.800><c> data</c><00:00:45.039><c> sets</c><00:00:45.440><c> became</c><00:00:45.920><c> too</c><00:00:46.160><c> big</c><00:00:46.559><c> to</c><00:00:46.719><c> be</c>

00:00:46.869 --> 00:00:46.879 align:start position:0%
and when data sets became too big to be
 

00:00:46.879 --> 00:00:51.270 align:start position:0%
and when data sets became too big to be
computed<00:00:47.520><c> even</c><00:00:47.760><c> once</c><00:00:48.640><c> things</c><00:00:49.280><c> had</c><00:00:49.440><c> to</c><00:00:49.680><c> change</c>

00:00:51.270 --> 00:00:51.280 align:start position:0%
computed even once things had to change
 

00:00:51.280 --> 00:00:53.189 align:start position:0%
computed even once things had to change
my<00:00:51.440><c> name</c><00:00:51.600><c> is</c><00:00:51.760><c> adina</c><00:00:52.239><c> and</c><00:00:52.320><c> my</c><00:00:52.480><c> colleagues</c><00:00:52.879><c> and</c><00:00:52.960><c> i</c>

00:00:53.189 --> 00:00:53.199 align:start position:0%
my name is adina and my colleagues and i
 

00:00:53.199 --> 00:00:54.790 align:start position:0%
my name is adina and my colleagues and i
created<00:00:53.520><c> a</c><00:00:53.600><c> framework</c><00:00:54.000><c> to</c><00:00:54.160><c> not</c><00:00:54.399><c> only</c><00:00:54.559><c> make</c>

00:00:54.790 --> 00:00:54.800 align:start position:0%
created a framework to not only make
 

00:00:54.800 --> 00:00:56.470 align:start position:0%
created a framework to not only make
processing<00:00:55.280><c> of</c><00:00:55.440><c> large</c><00:00:55.680><c> scale</c><00:00:55.920><c> data</c><00:00:56.239><c> sets</c>

00:00:56.470 --> 00:00:56.480 align:start position:0%
processing of large scale data sets
 

00:00:56.480 --> 00:00:58.630 align:start position:0%
processing of large scale data sets
possible<00:00:57.199><c> but</c><00:00:57.360><c> its</c><00:00:57.520><c> outcomes</c><00:00:58.000><c> also</c><00:00:58.239><c> easily</c>

00:00:58.630 --> 00:00:58.640 align:start position:0%
possible but its outcomes also easily
 

00:00:58.640 --> 00:01:00.549 align:start position:0%
possible but its outcomes also easily
shareable<00:00:59.120><c> transparent</c><00:00:59.760><c> and</c><00:00:59.920><c> automatically</c>

00:01:00.549 --> 00:01:00.559 align:start position:0%
shareable transparent and automatically
 

00:01:00.559 --> 00:01:01.830 align:start position:0%
shareable transparent and automatically
recomputable

00:01:01.830 --> 00:01:01.840 align:start position:0%
recomputable
 

00:01:01.840 --> 00:01:03.590 align:start position:0%
recomputable
we<00:01:02.000><c> start</c><00:01:02.239><c> with</c><00:01:02.399><c> a</c><00:01:02.559><c> datalet</c><00:01:02.960><c> dataset</c><00:01:03.359><c> on</c><00:01:03.520><c> a</c>

00:01:03.590 --> 00:01:03.600 align:start position:0%
we start with a datalet dataset on a
 

00:01:03.600 --> 00:01:05.990 align:start position:0%
we start with a datalet dataset on a
computational<00:01:04.239><c> cluster</c><00:01:05.119><c> datasets</c><00:01:05.680><c> are</c><00:01:05.760><c> based</c>

00:01:05.990 --> 00:01:06.000 align:start position:0%
computational cluster datasets are based
 

00:01:06.000 --> 00:01:07.590 align:start position:0%
computational cluster datasets are based
on<00:01:06.080><c> git</c><00:01:06.240><c> repositories</c><00:01:06.960><c> but</c><00:01:07.119><c> can</c><00:01:07.280><c> version</c>

00:01:07.590 --> 00:01:07.600 align:start position:0%
on git repositories but can version
 

00:01:07.600 --> 00:01:09.670 align:start position:0%
on git repositories but can version
control<00:01:08.000><c> digital</c><00:01:08.320><c> files</c><00:01:08.720><c> of</c><00:01:08.880><c> any</c><00:01:09.040><c> size</c><00:01:09.439><c> such</c>

00:01:09.670 --> 00:01:09.680 align:start position:0%
control digital files of any size such
 

00:01:09.680 --> 00:01:11.510 align:start position:0%
control digital files of any size such
as<00:01:09.760><c> the</c><00:01:09.920><c> uk</c><00:01:10.159><c> biobank</c><00:01:10.640><c> data</c><00:01:11.119><c> which</c><00:01:11.360><c> we</c>

00:01:11.510 --> 00:01:11.520 align:start position:0%
as the uk biobank data which we
 

00:01:11.520 --> 00:01:14.149 align:start position:0%
as the uk biobank data which we
retrieved<00:01:12.000><c> using</c><00:01:12.240><c> data.uk</c><00:01:12.960><c> biobank</c><00:01:13.920><c> as</c><00:01:14.080><c> you</c>

00:01:14.149 --> 00:01:14.159 align:start position:0%
retrieved using data.uk biobank as you
 

00:01:14.159 --> 00:01:16.469 align:start position:0%
retrieved using data.uk biobank as you
can<00:01:14.400><c> see</c><00:01:14.560><c> here</c><00:01:15.200><c> datasets</c><00:01:15.759><c> can</c><00:01:15.920><c> contain</c><00:01:16.320><c> other</c>

00:01:16.469 --> 00:01:16.479 align:start position:0%
can see here datasets can contain other
 

00:01:16.479 --> 00:01:18.469 align:start position:0%
can see here datasets can contain other
datasets<00:01:17.200><c> this</c><00:01:17.439><c> is</c><00:01:17.520><c> useful</c><00:01:17.920><c> to</c><00:01:18.080><c> structure</c>

00:01:18.469 --> 00:01:18.479 align:start position:0%
datasets this is useful to structure
 

00:01:18.479 --> 00:01:20.469 align:start position:0%
datasets this is useful to structure
large<00:01:18.720><c> datasets</c><00:01:19.200><c> into</c><00:01:19.439><c> smaller</c><00:01:19.759><c> units</c><00:01:20.240><c> but</c>

00:01:20.469 --> 00:01:20.479 align:start position:0%
large datasets into smaller units but
 

00:01:20.479 --> 00:01:22.710 align:start position:0%
large datasets into smaller units but
also<00:01:20.880><c> to</c><00:01:21.040><c> link</c><00:01:21.280><c> datasets</c><00:01:21.759><c> as</c><00:01:21.920><c> dependencies</c><00:01:22.560><c> to</c>

00:01:22.710 --> 00:01:22.720 align:start position:0%
also to link datasets as dependencies to
 

00:01:22.720 --> 00:01:23.830 align:start position:0%
also to link datasets as dependencies to
one<00:01:22.880><c> another</c>

00:01:23.830 --> 00:01:23.840 align:start position:0%
one another
 

00:01:23.840 --> 00:01:26.230 align:start position:0%
one another
we<00:01:24.000><c> use</c><00:01:24.159><c> one</c><00:01:24.320><c> dataset</c><00:01:24.799><c> to</c><00:01:24.960><c> link</c><00:01:25.200><c> ukb</c><00:01:25.600><c> data</c><00:01:26.080><c> and</c>

00:01:26.230 --> 00:01:26.240 align:start position:0%
we use one dataset to link ukb data and
 

00:01:26.240 --> 00:01:27.990 align:start position:0%
we use one dataset to link ukb data and
a<00:01:26.320><c> software</c><00:01:26.640><c> dataset</c><00:01:27.119><c> with</c><00:01:27.280><c> a</c><00:01:27.360><c> computational</c>

00:01:27.990 --> 00:01:28.000 align:start position:0%
a software dataset with a computational
 

00:01:28.000 --> 00:01:29.429 align:start position:0%
a software dataset with a computational
pipeline<00:01:28.479><c> in</c><00:01:28.560><c> the</c><00:01:28.640><c> form</c><00:01:28.880><c> of</c><00:01:29.040><c> a</c><00:01:29.119><c> software</c>

00:01:29.429 --> 00:01:29.439 align:start position:0%
pipeline in the form of a software
 

00:01:29.439 --> 00:01:31.830 align:start position:0%
pipeline in the form of a software
container<00:01:29.920><c> as</c><00:01:30.079><c> analysis</c><00:01:30.560><c> dependencies</c>

00:01:31.830 --> 00:01:31.840 align:start position:0%
container as analysis dependencies
 

00:01:31.840 --> 00:01:33.830 align:start position:0%
container as analysis dependencies
datasets<00:01:32.400><c> can</c><00:01:32.560><c> drop</c><00:01:32.880><c> and</c><00:01:32.960><c> re-retrieve</c><00:01:33.600><c> file</c>

00:01:33.830 --> 00:01:33.840 align:start position:0%
datasets can drop and re-retrieve file
 

00:01:33.840 --> 00:01:36.149 align:start position:0%
datasets can drop and re-retrieve file
content<00:01:34.159><c> that</c><00:01:34.320><c> is</c><00:01:34.400><c> hosted</c><00:01:34.799><c> elsewhere</c><00:01:35.680><c> despite</c>

00:01:36.149 --> 00:01:36.159 align:start position:0%
content that is hosted elsewhere despite
 

00:01:36.159 --> 00:01:38.469 align:start position:0%
content that is hosted elsewhere despite
tracking<00:01:36.560><c> terabytes</c><00:01:37.119><c> of</c><00:01:37.200><c> data</c><00:01:37.759><c> datasets</c><00:01:38.320><c> can</c>

00:01:38.469 --> 00:01:38.479 align:start position:0%
tracking terabytes of data datasets can
 

00:01:38.479 --> 00:01:40.870 align:start position:0%
tracking terabytes of data datasets can
thus<00:01:38.720><c> be</c><00:01:38.960><c> tiny</c><00:01:39.280><c> in</c><00:01:39.439><c> size</c><00:01:40.159><c> this</c><00:01:40.400><c> feature</c><00:01:40.799><c> is</c>

00:01:40.870 --> 00:01:40.880 align:start position:0%
thus be tiny in size this feature is
 

00:01:40.880 --> 00:01:42.469 align:start position:0%
thus be tiny in size this feature is
used<00:01:41.200><c> to</c><00:01:41.360><c> create</c><00:01:41.600><c> hundreds</c><00:01:42.000><c> of</c><00:01:42.159><c> single</c>

00:01:42.469 --> 00:01:42.479 align:start position:0%
used to create hundreds of single
 

00:01:42.479 --> 00:01:44.710 align:start position:0%
used to create hundreds of single
subject<00:01:42.880><c> analysis</c><00:01:43.600><c> that</c><00:01:43.840><c> only</c><00:01:44.159><c> retrieve</c><00:01:44.560><c> the</c>

00:01:44.710 --> 00:01:44.720 align:start position:0%
subject analysis that only retrieve the
 

00:01:44.720 --> 00:01:46.149 align:start position:0%
subject analysis that only retrieve the
files<00:01:45.040><c> they</c><00:01:45.200><c> need</c>

00:01:46.149 --> 00:01:46.159 align:start position:0%
files they need
 

00:01:46.159 --> 00:01:48.550 align:start position:0%
files they need
at<00:01:46.399><c> analysis</c><00:01:46.880><c> execution</c><00:01:47.680><c> a</c><00:01:47.840><c> job</c><00:01:48.079><c> scheduler</c>

00:01:48.550 --> 00:01:48.560 align:start position:0%
at analysis execution a job scheduler
 

00:01:48.560 --> 00:01:50.469 align:start position:0%
at analysis execution a job scheduler
distributes<00:01:49.119><c> the</c><00:01:49.200><c> analysis</c><00:01:49.759><c> over</c><00:01:50.000><c> available</c>

00:01:50.469 --> 00:01:50.479 align:start position:0%
distributes the analysis over available
 

00:01:50.479 --> 00:01:52.710 align:start position:0%
distributes the analysis over available
compute<00:01:50.880><c> nodes</c><00:01:51.280><c> each</c><00:01:51.520><c> compute</c><00:01:51.920><c> node</c><00:01:52.399><c> clones</c>

00:01:52.710 --> 00:01:52.720 align:start position:0%
compute nodes each compute node clones
 

00:01:52.720 --> 00:01:54.230 align:start position:0%
compute nodes each compute node clones
the<00:01:52.799><c> topmost</c><00:01:53.280><c> dataset</c><00:01:53.759><c> to</c><00:01:53.840><c> create</c><00:01:54.159><c> a</c>

00:01:54.230 --> 00:01:54.240 align:start position:0%
the topmost dataset to create a
 

00:01:54.240 --> 00:01:56.469 align:start position:0%
the topmost dataset to create a
short-lived<00:01:54.799><c> ephemeral</c><00:01:55.280><c> clone</c><00:01:55.840><c> resulting</c><00:01:56.320><c> in</c>

00:01:56.469 --> 00:01:56.479 align:start position:0%
short-lived ephemeral clone resulting in
 

00:01:56.479 --> 00:01:58.789 align:start position:0%
short-lived ephemeral clone resulting in
a<00:01:56.560><c> network</c><00:01:56.960><c> of</c><00:01:57.040><c> temporary</c><00:01:57.600><c> dataset</c><00:01:58.000><c> copies</c>

00:01:58.789 --> 00:01:58.799 align:start position:0%
a network of temporary dataset copies
 

00:01:58.799 --> 00:02:01.030 align:start position:0%
a network of temporary dataset copies
each<00:01:59.040><c> ephemeral</c><00:01:59.600><c> clone</c><00:02:00.159><c> is</c><00:02:00.320><c> tasked</c><00:02:00.640><c> with</c><00:02:00.799><c> one</c>

00:02:01.030 --> 00:02:01.040 align:start position:0%
each ephemeral clone is tasked with one
 

00:02:01.040 --> 00:02:03.749 align:start position:0%
each ephemeral clone is tasked with one
subset<00:02:01.439><c> of</c><00:02:01.600><c> analysis</c><00:02:02.079><c> execution</c><00:02:03.119><c> this</c><00:02:03.360><c> job</c><00:02:03.680><c> is</c>

00:02:03.749 --> 00:02:03.759 align:start position:0%
subset of analysis execution this job is
 

00:02:03.759 --> 00:02:05.590 align:start position:0%
subset of analysis execution this job is
performed<00:02:04.240><c> with</c><00:02:04.399><c> a</c><00:02:04.479><c> data</c><00:02:04.719><c> that</c><00:02:04.880><c> contains</c><00:02:05.360><c> one</c>

00:02:05.590 --> 00:02:05.600 align:start position:0%
performed with a data that contains one
 

00:02:05.600 --> 00:02:07.670 align:start position:0%
performed with a data that contains one
call<00:02:06.240><c> its</c><00:02:06.479><c> advantage</c><00:02:06.960><c> is</c><00:02:07.119><c> that</c><00:02:07.280><c> the</c><00:02:07.439><c> full</c>

00:02:07.670 --> 00:02:07.680 align:start position:0%
call its advantage is that the full
 

00:02:07.680 --> 00:02:09.589 align:start position:0%
call its advantage is that the full
digital<00:02:08.160><c> analysis</c><00:02:08.640><c> provenance</c><00:02:09.119><c> is</c><00:02:09.200><c> captured</c>

00:02:09.589 --> 00:02:09.599 align:start position:0%
digital analysis provenance is captured
 

00:02:09.599 --> 00:02:11.270 align:start position:0%
digital analysis provenance is captured
in<00:02:09.759><c> a</c><00:02:09.840><c> structured</c><00:02:10.239><c> record</c><00:02:10.560><c> that</c><00:02:10.720><c> can</c><00:02:10.879><c> be</c><00:02:11.039><c> used</c>

00:02:11.270 --> 00:02:11.280 align:start position:0%
in a structured record that can be used
 

00:02:11.280 --> 00:02:13.190 align:start position:0%
in a structured record that can be used
for<00:02:11.360><c> automatic</c><00:02:11.840><c> re-execution</c>

00:02:13.190 --> 00:02:13.200 align:start position:0%
for automatic re-execution
 

00:02:13.200 --> 00:02:14.710 align:start position:0%
for automatic re-execution
provenance<00:02:13.680><c> and</c><00:02:13.760><c> computed</c><00:02:14.239><c> results</c><00:02:14.640><c> are</c>

00:02:14.710 --> 00:02:14.720 align:start position:0%
provenance and computed results are
 

00:02:14.720 --> 00:02:17.190 align:start position:0%
provenance and computed results are
saved<00:02:15.280><c> pushed</c><00:02:16.080><c> and</c><00:02:16.400><c> when</c><00:02:16.640><c> all</c><00:02:16.800><c> jobs</c><00:02:17.040><c> are</c>

00:02:17.190 --> 00:02:17.200 align:start position:0%
saved pushed and when all jobs are
 

00:02:17.200 --> 00:02:19.350 align:start position:0%
saved pushed and when all jobs are
finished<00:02:17.840><c> merged</c><00:02:18.319><c> back</c><00:02:18.560><c> into</c><00:02:18.800><c> the</c><00:02:18.959><c> central</c>

00:02:19.350 --> 00:02:19.360 align:start position:0%
finished merged back into the central
 

00:02:19.360 --> 00:02:20.470 align:start position:0%
finished merged back into the central
dataset

00:02:20.470 --> 00:02:20.480 align:start position:0%
dataset
 

00:02:20.480 --> 00:02:21.990 align:start position:0%
dataset
throughout<00:02:20.879><c> the</c><00:02:21.040><c> process</c><00:02:21.440><c> a</c><00:02:21.599><c> special</c>

00:02:21.990 --> 00:02:22.000 align:start position:0%
throughout the process a special
 

00:02:22.000 --> 00:02:23.670 align:start position:0%
throughout the process a special
internal<00:02:22.480><c> data</c><00:02:22.720><c> set</c><00:02:22.959><c> representation</c>

00:02:23.670 --> 00:02:23.680 align:start position:0%
internal data set representation
 

00:02:23.680 --> 00:02:25.990 align:start position:0%
internal data set representation
minimizes<00:02:24.239><c> disk</c><00:02:24.560><c> space</c><00:02:24.879><c> and</c><00:02:24.959><c> inode</c><00:02:25.360><c> usage</c><00:02:25.840><c> and</c>

00:02:25.990 --> 00:02:26.000 align:start position:0%
minimizes disk space and inode usage and
 

00:02:26.000 --> 00:02:27.510 align:start position:0%
minimizes disk space and inode usage and
provides<00:02:26.400><c> optional</c><00:02:26.800><c> encryption</c><00:02:27.200><c> during</c>

00:02:27.510 --> 00:02:27.520 align:start position:0%
provides optional encryption during
 

00:02:27.520 --> 00:02:29.830 align:start position:0%
provides optional encryption during
transport<00:02:28.480><c> this</c><00:02:28.720><c> allows</c><00:02:29.040><c> processing</c><00:02:29.440><c> of</c><00:02:29.599><c> data</c>

00:02:29.830 --> 00:02:29.840 align:start position:0%
transport this allows processing of data
 

00:02:29.840 --> 00:02:31.509 align:start position:0%
transport this allows processing of data
sets<00:02:30.080><c> that</c><00:02:30.239><c> are</c><00:02:30.319><c> larger</c><00:02:30.720><c> than</c><00:02:30.879><c> the</c><00:02:31.040><c> available</c>

00:02:31.509 --> 00:02:31.519 align:start position:0%
sets that are larger than the available
 

00:02:31.519 --> 00:02:33.270 align:start position:0%
sets that are larger than the available
resources<00:02:32.080><c> with</c><00:02:32.319><c> only</c><00:02:32.560><c> minimal</c><00:02:32.879><c> software</c>

00:02:33.270 --> 00:02:33.280 align:start position:0%
resources with only minimal software
 

00:02:33.280 --> 00:02:35.270 align:start position:0%
resources with only minimal software
requirements<00:02:33.840><c> on</c><00:02:34.000><c> the</c><00:02:34.080><c> server</c><00:02:34.400><c> side</c>

00:02:35.270 --> 00:02:35.280 align:start position:0%
requirements on the server side
 

00:02:35.280 --> 00:02:37.270 align:start position:0%
requirements on the server side
because<00:02:35.519><c> data</c><00:02:35.920><c> sets</c><00:02:36.239><c> can</c><00:02:36.480><c> be</c><00:02:36.640><c> easily</c><00:02:37.040><c> shared</c>

00:02:37.270 --> 00:02:37.280 align:start position:0%
because data sets can be easily shared
 

00:02:37.280 --> 00:02:39.190 align:start position:0%
because data sets can be easily shared
with<00:02:37.440><c> appropriate</c><00:02:38.000><c> audiences</c><00:02:38.560><c> the</c><00:02:38.720><c> resulting</c>

00:02:39.190 --> 00:02:39.200 align:start position:0%
with appropriate audiences the resulting
 

00:02:39.200 --> 00:02:40.790 align:start position:0%
with appropriate audiences the resulting
data<00:02:39.440><c> sets</c><00:02:39.680><c> can</c><00:02:39.840><c> be</c><00:02:40.000><c> distributed</c><00:02:40.640><c> in</c><00:02:40.720><c> a</c>

00:02:40.790 --> 00:02:40.800 align:start position:0%
data sets can be distributed in a
 

00:02:40.800 --> 00:02:42.869 align:start position:0%
data sets can be distributed in a
streamlined<00:02:41.360><c> transparent</c><00:02:42.080><c> and</c><00:02:42.239><c> reusable</c>

00:02:42.869 --> 00:02:42.879 align:start position:0%
streamlined transparent and reusable
 

00:02:42.879 --> 00:02:45.430 align:start position:0%
streamlined transparent and reusable
manner<00:02:43.519><c> as</c><00:02:43.840><c> jobs</c><00:02:44.080><c> were</c><00:02:44.239><c> computed</c><00:02:44.720><c> in</c><00:02:44.879><c> isolated</c>

00:02:45.430 --> 00:02:45.440 align:start position:0%
manner as jobs were computed in isolated
 

00:02:45.440 --> 00:02:46.869 align:start position:0%
manner as jobs were computed in isolated
compute<00:02:45.760><c> environments</c><00:02:46.560><c> they</c><00:02:46.800><c> are</c>

00:02:46.869 --> 00:02:46.879 align:start position:0%
compute environments they are
 

00:02:46.879 --> 00:02:48.550 align:start position:0%
compute environments they are
automatically<00:02:47.519><c> portable</c><00:02:48.080><c> to</c><00:02:48.319><c> other</c>

00:02:48.550 --> 00:02:48.560 align:start position:0%
automatically portable to other
 

00:02:48.560 --> 00:02:50.550 align:start position:0%
automatically portable to other
infrastructure<00:02:49.519><c> as</c><00:02:49.680><c> long</c><00:02:49.840><c> as</c><00:02:50.000><c> data</c><00:02:50.239><c> light</c><00:02:50.400><c> and</c>

00:02:50.550 --> 00:02:50.560 align:start position:0%
infrastructure as long as data light and
 

00:02:50.560 --> 00:02:52.150 align:start position:0%
infrastructure as long as data light and
the<00:02:50.640><c> employed</c><00:02:51.040><c> container</c><00:02:51.440><c> technology</c><00:02:52.000><c> are</c>

00:02:52.150 --> 00:02:52.160 align:start position:0%
the employed container technology are
 

00:02:52.160 --> 00:02:54.470 align:start position:0%
the employed container technology are
available<00:02:52.959><c> whoever</c><00:02:53.360><c> obtains</c><00:02:53.680><c> this</c><00:02:53.840><c> data</c><00:02:54.160><c> set</c>

00:02:54.470 --> 00:02:54.480 align:start position:0%
available whoever obtains this data set
 

00:02:54.480 --> 00:02:56.630 align:start position:0%
available whoever obtains this data set
can<00:02:54.640><c> thus</c><00:02:54.879><c> recompute</c><00:02:55.440><c> each</c><00:02:55.599><c> individual</c><00:02:56.239><c> job</c>

00:02:56.630 --> 00:02:56.640 align:start position:0%
can thus recompute each individual job
 

00:02:56.640 --> 00:03:00.680 align:start position:0%
can thus recompute each individual job
on<00:02:56.720><c> their</c><00:02:56.959><c> own</c><00:02:57.120><c> computer</c><00:02:57.680><c> automatically</c>

